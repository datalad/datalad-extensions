# This file is autogenerated from extensions.yaml using scripts/generate.
# See CONTRIBUTING.md for more information

name: test-datalad_crawler

on:
  # Trigger the workflow on pull request,
  # but only if any given extension workflow was modified
  pull_request:
    paths:
      - '.github/workflows/test-datalad_crawler.yaml'
  # all should be triggered on cron
  schedule:
    - cron: '30 01 * * *'

jobs:
  test:

    runs-on: ubuntu-latest

    strategy:
      fail-fast: false

    steps:
    - name: Set up system
      shell: bash
      run: |
        bash <(wget -q -O- http://neuro.debian.net/_files/neurodebian-travis.sh)
        sudo apt-get update -qq
        sudo apt-get install eatmydata
        sudo eatmydata apt-get install git-annex-standalone
    - name: Set up environment
      run: |
        git config --global user.email "test@github.land"
        git config --global user.name "GitHub Almighty"
    - uses: actions/checkout@v1
    - name: Set up Python 3.7
      uses: actions/setup-python@v1
      with:
        python-version: 3.7
    - name: Install DataLad (master)
      run: |
        python -m pip install --upgrade pip
        pip install https://github.com/datalad/datalad/archive/master.zip
    - name: Install datalad_crawler extension from PyPI ([devel])
      run: |
        pip install datalad_crawler[devel]
    - name: Install nose et al (just in case!)
      # Note, that we need nose even in case of pytest, due to test helpers
      # depending on it.
      run: |
        pip install nose vcrpy mock

    - name: WTF!?
      run: |
        datalad wtf
    - name: datalad_crawler tests
      run: |
        mkdir -p __testhome__
        cd __testhome__

        python -m nose -s -v --with-cov --cover-package datalad datalad_crawler
